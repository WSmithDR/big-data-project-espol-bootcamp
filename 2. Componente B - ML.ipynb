{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Install required packages\n",
    "%pip install pyspark pandas numpy duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Import libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when, regexp_replace, split, size, year, udf\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRatingPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Load and prepare data\n",
    "# Connect to DuckDB and get movies data\n",
    "conn = duckdb.connect('grupo2_bigdata.db')\n",
    "\n",
    "# Query to join MOVIES and USER_REVIEWS to get relevant features\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.*,\n",
    "    AVG(r.rating) as avg_rating,\n",
    "    COUNT(r.rating) as review_count\n",
    "FROM MOVIES m\n",
    "LEFT JOIN USER_REVIEWS r ON m.movieId = r.movieId\n",
    "GROUP BY m.movieId, m.title, m.originalTitle, m.genres, \n",
    "         m.originalLanguage, m.budget, m.revenue, m.runtime,\n",
    "         m.releaseDate\n",
    "LIMIT 1000  -- Remove this limit after testing\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and convert to pandas\n",
    "movies_df = conn.execute(query).fetchdf()\n",
    "conn.close()\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(movies_df)\n",
    "\n",
    "# Show schema and sample data\n",
    "print(\"DataFrame Schema:\")\n",
    "spark_df.printSchema()\n",
    "print(\"\\nSample Data:\")\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Feature Engineering (Fixed)\n",
    "def prepare_features(df):\n",
    "    # 1. Handle genres (convert to array and get count)\n",
    "    df = df.withColumn('genre_count', size(split(col('genres'), '\\\\|')))\n",
    "    \n",
    "    # 2. Convert dates to year\n",
    "    df = df.withColumn('release_year', year(col('releaseDate')))\n",
    "    \n",
    "    # 3. Handle numerical features\n",
    "    numeric_features = ['budget', 'revenue', 'runtime', 'review_count', \n",
    "                       'genre_count', 'release_year']\n",
    "    \n",
    "    # 4. Handle categorical features\n",
    "    categorical_features = ['originalLanguage']\n",
    "    \n",
    "    # Create indexers for categorical features\n",
    "    indexers = [StringIndexer(inputCol=feature_name, outputCol=feature_name+\"_idx\", handleInvalid=\"keep\") \n",
    "                for feature_name in categorical_features]\n",
    "    \n",
    "    # Create feature vector\n",
    "    numeric_cols = [feature_name + \"_scaled\" for feature_name in numeric_features]\n",
    "    categorical_cols = [feature_name + \"_idx\" for feature_name in categorical_features]\n",
    "    feature_cols = numeric_cols + categorical_cols\n",
    "    \n",
    "    # Create assemblers and scalers\n",
    "    assembler1 = VectorAssembler(inputCols=numeric_features, outputCol=\"numeric_features\")\n",
    "    scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\")\n",
    "    \n",
    "    # Create the split_features function\n",
    "    def split_features(vector_col, i):\n",
    "        return vector_col[i]\n",
    "    \n",
    "    # Create pipeline stages\n",
    "    stages = indexers + [assembler1, scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    transformed_df = pipeline.fit(df).transform(df)\n",
    "    \n",
    "    # Split the scaled features into individual columns\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import DoubleType\n",
    "    \n",
    "    for i, feature_name in enumerate(numeric_features):\n",
    "        get_item_udf = udf(lambda v: float(v[i]), DoubleType())\n",
    "        transformed_df = transformed_df.withColumn(\n",
    "            feature_name + \"_scaled\", \n",
    "            get_item_udf(col(\"scaled_numeric_features\"))\n",
    "        )\n",
    "    \n",
    "    # Create final feature vector\n",
    "    final_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    final_df = final_assembler.transform(transformed_df)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Apply feature engineering\n",
    "try:\n",
    "    featured_df = prepare_features(spark_df)\n",
    "    print(\"Feature engineering completed successfully!\")\n",
    "    print(\"\\nTransformed DataFrame:\")\n",
    "    featured_df.select(\"features\", \"avg_rating\").show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"Error during feature engineering:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Split Data\n",
    "# Split into training and testing sets\n",
    "train_data, test_data = featured_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Training Set Size:\", train_data.count())\n",
    "print(\"Testing Set Size:\", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Train Model\n",
    "# Create and train Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"avg_rating\",\n",
    "    numTrees=20,\n",
    "    maxDepth=10\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample Predictions:\")\n",
    "predictions.select(\"avg_rating\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - Evaluate Model\n",
    "# Calculate evaluation metrics\n",
    "evaluator = RegressionEvaluator(labelCol=\"avg_rating\", predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.3f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
    "print(f\"R-squared (R2): {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - Feature Importance\n",
    "# Get feature importance\n",
    "feature_importance = model.featureImportances\n",
    "feature_cols = ['budget', 'revenue', 'runtime', 'review_count', \n",
    "                'genre_count', 'release_year', 'originalLanguage_idx']\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importance.toArray()\n",
    "})\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 - Save Model\n",
    "# Save the model\n",
    "model_path = \"movie_rating_prediction_model\"\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
